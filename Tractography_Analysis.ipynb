{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# Script to analyse tractography-related statistics\n",
    "# ==================================================================\n",
    "import os\n",
    "import sys\n",
    "import glob #for wildcard matching\n",
    "import collections #for ordered dictionary\n",
    "import time\n",
    "import subprocess\n",
    "import nibabel as nib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All is well\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Anthony's notes\n",
    "- modules that needs to be loaded:\n",
    "    - qbatch/git\n",
    "    - FSL\n",
    "\n",
    "- during the initiation of the idx and pct file paths, it is ASSUMED that all input paths will:\n",
    "    - have a perfect correspondence of pct to idx paths for each subject\n",
    "        - both will appear in the same ordered after being sorted in the lists\n",
    "    - the second-to-last part deliminated by \"/\" will be the subject ID for both files\n",
    "- input file path assumptions\n",
    "    - there will be no period character (\".\") OTHER than for at the end\n",
    "    - there are two total period characters: \"filename.nii.gz\"\n",
    "\"\"\"\n",
    "print \"All is well\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# Setup files and directories\n",
    "# ==================================================================\n",
    "\n",
    "\"\"\" Note on directory hierarchy for the input, reference and transform files \n",
    "parent_dir\n",
    "    |\n",
    "    +-- subj_dir_1 (as indicated by dir_wc)\n",
    "    |        |\n",
    "    |        +- file_of_interest (as indicated by *_file_name)\n",
    "    |\n",
    "    +-- subj_dir_2 ...\n",
    "\"\"\"\n",
    "\n",
    "#Variable indication how many component parcellation we want to use\n",
    "SEG_NUM = 5 \n",
    "# ========================================\n",
    "# Input files for fslmaths and fslstats\n",
    "# ========================================\n",
    "in_parent_dir='/data/chamal/projects/anthony/nmf_parcellation/cortical_tractMap/seg'+str(SEG_NUM)+'_tract2voxel_probability_labels/model_space'\n",
    "in_dir_wc='[0-9][0-9][0-9][0-9][0-9][0-9]'\n",
    "idx_file_name = '*_region_seg_idx_modelSpace.nii.gz'\n",
    "pct_file_name = '*_region_seg_pct_modelSpace.nii.gz' \n",
    "\n",
    "# ========================================\n",
    "# Intermediate files\n",
    "# ========================================\n",
    "inter_parent_dir='/data/chamal/projects/anthony/nmf_parcellation/cortical_tractMap/seg'+str(SEG_NUM)+'_tract2voxel_probability_labels/tractAnalysis_model_space'\n",
    "csvDump_filename=\"seg\"+str(SEG_NUM)+\"_avgComponentWinPct_dump.csv\"\n",
    "\n",
    "# ========================================\n",
    "# JobDoc files\n",
    "# ========================================\n",
    "jobDoc_dir='/data/chamal/projects/anthony/qbatch_jobDocs/track_analysis_related'\n",
    "jobList_path=os.path.join(jobDoc_dir,'autoSubmitted_seg%d_tractAnalysis.sub'%SEG_NUM)\n",
    "jobScript_stdev_name=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parcellation component quantity is: 5\n",
      "idx_file_list length: 100\n",
      "pct_file_list length: 100\n",
      "\n",
      "100 subjects were initiated into the subjFile_Paths dictionary:\n",
      "100307 100408 101006 101107 101410 101915 102008 102311 102816 103111 105014 105115 105216 106016 106521 107321 107422 108121 108323 108525 109123 110411 111312 111413 111716 113215 113821 113922 114419 115320 116524 117122 118528 118730 118932 119833 120212 120515 121618 122317 122620 123117 123420 123925 124422 125525 126325 126628 127630 127933 128127 128632 129028 130013 130316 131217 131722 132118 133019 133827 134324 135225 135528 136227 136833 137027 138534 139233 140925 144832 146432 147030 147737 148840 148941 149741 150423 150524 150726 151223 151526 151627 151728 153833 154734 154835 154936 156233 156637 157437 158136 159239 159340 160830 161327 161630 162228 162733 163331 164030 \n",
      "\n",
      "Hopefully the above looks right!\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# Initiate paths of the two input files\n",
    "# ==================================================================\n",
    "\"\"\"Dictionary hierarchy\n",
    "subjFile_Paths\n",
    "    |\n",
    "    [\"100307\"]\n",
    "    |    |\n",
    "    |    [\"idx\"] -- path to 100307's idx file\n",
    "    |    [\"pct\"] -- path to 100307's pct file\n",
    "    |\n",
    "    [\"100408\"]\n",
    "    ...\n",
    "\"\"\"\n",
    "#Find all files on paths\n",
    "idx_file_list=glob.glob(os.path.join(in_parent_dir,in_dir_wc,idx_file_name))\n",
    "pct_file_list=glob.glob(os.path.join(in_parent_dir,in_dir_wc,pct_file_name))\n",
    "#Sort the two lists\n",
    "idx_file_list.sort()\n",
    "pct_file_list.sort()\n",
    "#Store the two files into a sorted dictionary\n",
    "subjFile_paths = collections.OrderedDict([]) #Initiate ordered dictionary\n",
    "for idx_file_path, pct_file_path in zip(idx_file_list, pct_file_list):\n",
    "    #Store the file paths in a temp dictionary\n",
    "    temp_dict = {}\n",
    "    temp_dict['idx'] = idx_file_path\n",
    "    temp_dict['pct'] = pct_file_path\n",
    "    #Check for subject ID and that they match up\n",
    "    temp_subj_ID = idx_file_path.split('/')[-2]\n",
    "    if pct_file_path.split('/')[-2] != temp_subj_ID: #If the two file IDs do not match\n",
    "        print \"Something went wrong in the pct and idx file matching for ID: %s\" % (temp_subj_ID)\n",
    "    #Add the temporary dictionary to the sorted dictionary\n",
    "    subjFile_paths[temp_subj_ID] = temp_dict\n",
    "\n",
    "#Let user know \n",
    "print \"The parcellation component quantity is: %d\" % SEG_NUM\n",
    "print \"idx_file_list length: %d\" % len(idx_file_list)\n",
    "print \"pct_file_list length: %d\" % len(pct_file_list)\n",
    "print \"\\n%d subjects were initiated into the subjFile_Paths dictionary:\" % len(subjFile_paths)\n",
    "for key in subjFile_paths:\n",
    "    print key,\n",
    "print \"\\n\\nHopefully the above looks right!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate directory found: /data/chamal/projects/anthony/nmf_parcellation/cortical_tractMap/seg5_tract2voxel_probability_labels/tractAnalysis_model_space\n",
      "\n",
      "Opening and writing to joblist file: /data/chamal/projects/anthony/qbatch_jobDocs/track_analysis_related/autoSubmitted_seg5_tractAnalysis.sub\n",
      "\n",
      "Finished writing to joblist file\n",
      "\n",
      "Submit status: False\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# Generate the jobs needed to calculate everyone's component average\n",
    "# ==================================================================\n",
    "\n",
    "#Qbatch specifics\n",
    "SUBMIT = False\n",
    "CHUNKSIZE = '3' #Need each 3 lines to run sequentially \n",
    "WALLTIME = '00:30:00'\n",
    "\n",
    "\n",
    "#Check to see if the output parent directory is present, if not create it\n",
    "if os.path.isdir(inter_parent_dir):\n",
    "    print \"Intermediate directory found: %s\" % inter_parent_dir\n",
    "else:\n",
    "    os.mkdir(inter_parent_dir)\n",
    "    print \"Intermediate directory not found, created one at: %s\"% inter_parent_dir\n",
    "    \n",
    "#Create the path to the .csv file to dump all of the pct averages\n",
    "csvDump_path = os.path.join(inter_parent_dir, csvDump_filename)\n",
    "    \n",
    "#Open the jobList document to write to\n",
    "print \"\\nOpening and writing to joblist file: %s\" % jobList_path\n",
    "jobOut = open(jobList_path, 'w')\n",
    "\n",
    "#Iterate through each subject\n",
    "for subj in subjFile_paths:\n",
    "    #Check to see if the output directory is present, if not create it\n",
    "    subj_dir_path = os.path.join(inter_parent_dir, subj)\n",
    "    if not os.path.isdir(subj_dir_path):\n",
    "        os.mkdir(subj_dir_path)\n",
    "    \n",
    "    #Get the input files\n",
    "    idxPath = subjFile_paths[subj]['idx']\n",
    "    pctPath = subjFile_paths[subj]['pct']\n",
    "    \n",
    "    #Iterate through each component\n",
    "    for comp_num in range(1,SEG_NUM+1):\n",
    "        #Generate the output file paths\n",
    "        binarizedComp_name = idxPath.split('/')[-1].split('.')[0]+\"_comp\"+str(comp_num)+\"_binarized.nii.gz\"\n",
    "        binarizedComp_outPath = os.path.join(subj_dir_path, binarizedComp_name )\n",
    "        pct_comp_name = pctPath.split('/')[-1].split('.')[0]+\"_comp\"+str(comp_num)+\".nii.gz\"\n",
    "        pct_comp_outPath = os.path.join(subj_dir_path, pct_comp_name )\n",
    "\n",
    "        #Generate the fsl commands\n",
    "        binarization='fslmaths %s -thr %d -thr %d -bin %s' % (idxPath, comp_num,comp_num, binarizedComp_outPath)\n",
    "        multiplication='fslmaths %s -mul %s %s' % (pctPath, binarizedComp_outPath, pct_comp_outPath)\n",
    "        calAvg='echo \"%s,%s,`fslstats %s -M`\" >> %s' % (subj, \"comp_\"+str(comp_num), pct_comp_outPath, csvDump_path)\n",
    "        \n",
    "        #Write the commands to the jobList file\n",
    "        jobOut.write(binarization+\"\\n\")\n",
    "        jobOut.write(multiplication+\"\\n\")\n",
    "        jobOut.write(calAvg+\"\\n\")\n",
    "\n",
    "\n",
    "#Close the jobList output stream\n",
    "jobOut.close()\n",
    "print \"\\nFinished writing to joblist file\"\n",
    "\n",
    "#Optionally submit the file to cluster\n",
    "print \"\\nSubmit status: %s\" % str(SUBMIT)\n",
    "if SUBMIT:\n",
    "    print \"Submitting job...\"\n",
    "    cmd = ['qbatch','-w',WALLTIME, '-c',CHUNKSIZE, jobList_path]\n",
    "    subprocess.call(cmd)\n",
    "\n",
    "print \"Done\"    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
